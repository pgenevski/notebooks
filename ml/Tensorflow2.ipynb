{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Tensorflow is a library for high-performance (parallel, GPU accelerated ...) computation that also supports automatic reverse-mode differentiation (aka backpropagation).\n",
    "\n",
    "References:\n",
    "\n",
    "[[1] Automatic differentiation](http://www.columbia.edu/~ahd2125/post/2015/12/5/)\n",
    "\n",
    "[[2] Backpropagation, Chris Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "\n",
    "[[3] Reverse mode automatic differentiation](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation)\n",
    "\n",
    "\n",
    "## Runtimes\n",
    "Runtime environments can be local (CPU, GPU ..) or remote (distributed training). Here is how to get a list of the available execution environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:CPU:0', '/device:XLA_CPU:0']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "[x.name for x in  device_lib.list_local_devices()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both CPU and GPU are available, Tensorflow will use the GPU (hoping for better performance). This is true even in eager mode. Device placement context managers also work as in TF1 `with tf.device(\"/gpu:0\"):`\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Tensors are either simple values or (the output of) operations.\n",
    "\n",
    "### Values (tensors)\n",
    "Values are always n-rank tensors (scalar, vector, matrix ...) with a given shape and dtype. Depending on their role in the computation they can be:\n",
    "* **[`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant)** - these tensors are not supposed to change\n",
    "\n",
    "* **[`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)** - these are the model parameters that will be optimized. Usually initialized randomly at the beginning of the computation. Because of the automatic differentiation, Tensorflow can automatically compute the gradients of a specified function (e.g. loss) with respect to the variables (unless they are marked as trainable=False).\n",
    "\n",
    "Note: There are no placeholders in TF2, because the graph is built dynamically (define by run as in pytorch).\n",
    "\n",
    "Note: Strictly speaking, in tensorflow terminology, the above three are also operations that return tensors.\n",
    "\n",
    "\n",
    "### Operations\n",
    "[Operations](https://www.tensorflow.org/api_docs/python/tf/Operation) are used to perform computations with tensors. They  take $m \\in [1, \\infty)$ tensors as input and produce $n \\in [1,\\infty)$ tensors as output. Inputs to operations can be Values or outputs from other operations. Each operation usually has the notion of a batch. This is a data dimension that can be used for parallelization. This has two effects\n",
    "\n",
    "1. The computations are done in parallel\n",
    "2. The gradients are accumulated and averaged over the (mini) batch.\n",
    "\n",
    "For example the shape of a batch of images would be (batch_size, width, height, channels).\n",
    "\n",
    "## Eager mode vs graph mode\n",
    "\n",
    "\n",
    "Eager execution is the default mode in TF2. Tensors behave much like numpy nd arrays.This means that a graph is not created. This has several implications:\n",
    "\n",
    "- Performance may be lower\n",
    "- You can't save the graph (as in keras.Model and Checkpointable)\n",
    "- You can't write the graph for tensorboard visualization\n",
    "\n",
    "If you want to do computations in a single graph, use ` @tf.function` decorator (e.g. on the train_step function). It will apply autograph on the function and ones down the call tree, in order to build a single, callable graph (also converting python operators to graph ops). Inside the annotated function, all tensors will be treated as symbolic.\n",
    "The graph is created during the first run of the annotated function and then cached. If the function is called with again with different shape/dtype inputs, another graph will be created and cached for this combination of shape/dtype. This may lead to memory exhaustion if the function is called with lots of different input shape/dtypes. \n",
    "\n",
    "> CAUTION: Same applies when calling the function with scalars, but then the graph is cached for each scalar value(s), not shapes and dtypes.\n",
    "\n",
    "It is recommended to puth the `tf.data` calls inside a ` @tf.function` too.\n",
    "\n",
    "> There are no placeholders in TF2. Just feed the values as function arguments and return the \"fetches\".\n",
    "\n",
    "> There are no explicit [Sessions](https://www.tensorflow.org/api_docs/python/tf/Session#run) in TF2. It is managed by TF instead.\n",
    "\n",
    "Print statements in ` @tf.function` will only be executed once, when the function(s) are being traced. The result of the tracing is a compiled graph that doesn't run python and hence print statements are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager1:  False\n",
      "Inside2:  1 2 Tensor(\"Linear_function/Add_1:0\", shape=(), dtype=int32)\n",
      "Eager2:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func1(a,b):\n",
    "    print('Eager1: ',tf.executing_eagerly())\n",
    "    return tf.add(a,b) # if you don't use any tf ops, a graph will not be built\n",
    "\n",
    "@tf.function # this function and the ones it calls will be run in graph mode\n",
    "def func2(a,b):\n",
    "    with tf.name_scope(\"Linear_function\"): # name scopes help group ops visually in tensorboard\n",
    "        result = tf.add(b,func1(a,b))\n",
    "    \n",
    "    print('Inside2: ',a,b,result)\n",
    "    print('Eager2: ',tf.executing_eagerly())\n",
    "    \n",
    "    return result\n",
    "\n",
    "res = func2(1,2)\n",
    "# print(tf.autograph.to_code(func2.python_function))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=13>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = func2(3,5)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing graphs\n",
    "Tensorflow comes with a visualization tool called [**tensorboard**](https://youtu.be/eBbEDRsCmv4). Let's use this tool to visualize our graph.\n",
    "\n",
    "To do this, you either use the Keras TensorboardCallback, or you write the graph explicitly.\n",
    "\n",
    "## Writing graphs\n",
    "Graphs are written to an output folder using the `tf.summary` module. However, graph must be traced when it is called for the first time (immediately after definition).\n",
    "\n",
    "https://www.tensorflow.org/tensorboard/graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager1:  False\n",
      "Inside2:  8 5 Tensor(\"Linear_function/Add_1:0\", shape=(), dtype=int32)\n",
      "Eager2:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=18>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = \"tensorboard2\"\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "tf.summary.trace_on(graph=True)\n",
    "res = func2(8,5) # Call only one tf.function when tracing.\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(name=\"func2\",step=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir tensorboard2\n",
    "```\n",
    "\n",
    "Then open the URL from the console in a browser where you can see this graph. The names of the variables that we set above will appear in the graph.\n",
    "\n",
    "<!-- ![example_graph.png](example_graph.png \"Resulting graph\") -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating gradients\n",
    "\n",
    "As in TF1, gradients can be calculated with `tf.GradientTape`. It works in eager mode too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= <tf.Variable 'x:0' shape=() dtype=float32, numpy=5.0>\n",
      "y= tf.Tensor(35.0, shape=(), dtype=float32)\n",
      "dydx= [<tf.Tensor: shape=(), dtype=float32, numpy=5.0>]\n"
     ]
    }
   ],
   "source": [
    "# y = ax + b\n",
    "\n",
    "x = tf.Variable(5.0, name='x')\n",
    "\n",
    "def calculate(a,b):\n",
    "\n",
    "    with tf.GradientTape() as tape: # to calculate gradients you need variables and this tape\n",
    "        y = a*x + b\n",
    "    dydx = tape.gradient(y, [x])\n",
    "    return x,y,dydx\n",
    "    \n",
    "\n",
    "x_val,y_val,dydx_val = calculate(5,10)\n",
    "print('x=',x_val)\n",
    "print('y=',y_val)\n",
    "print('dydx=',dydx_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach is to encapsulate variables and operations in a Keras layer or Model. Both have the `trainable_variables` field. However, they are initialized laizily upon the first call to the layer/model or when build is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'conv2d_28/kernel:0' shape=(1, 1, 1, 1) dtype=float32, numpy=array([[[[0.32172024]]]], dtype=float32)>, <tf.Variable 'conv2d_28/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((1,2,2,1),dtype=np.float32)\n",
    "conv1 = tf.keras.layers.Conv2D(1,(1,1))\n",
    "#conv1.build(input_shape=(1,2,2,1)) # if we comment this, there will be no variables\n",
    "conv1(a) # unless we call the layer first\n",
    "print(conv1.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 4.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[2., 0.],\n",
       "        [0., 4.]], dtype=float32)>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "    x  = tf.constant([1.0, 2.0])\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "jacobian = g.jacobian(y, x)\n",
    "y, jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries\n",
    "\n",
    "Summaries are a way to collect and visualize tensors.\n",
    "\n",
    "https://itnext.io/how-to-use-tensorboard-5d82f8654496\n",
    "\n",
    "### Scalar\n",
    "The simplest summary is the scalar summary. It's purpose is to track the value of a scalar along one or more steps. It is often used to plot the evolution of the loss function and accuracy along the training process. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph() # clean up previous ops\n",
    "\n",
    "a = tf.constant(5.0, name='a')\n",
    "x = tf.placeholder(tf.float32, shape=(), name='x')\n",
    "y = a*x\n",
    "\n",
    "tf.summary.scalar(\"y_scalar_summary\", y)\n",
    "merged_summary = tf.summary.merge_all() # merge all summary ops into a single op so that we execute all\n",
    "\n",
    "data = np.random.randint(10,size=100)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    writer = tf.summary.FileWriter(\"tensorboard3\",session.graph)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for step,val in enumerate(data):\n",
    "        (y_val,s_val) = session.run((y,merged_summary), {x: val})\n",
    "        writer.add_summary(s_val,step) # Passing the step is important !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"scalar_summary.png\" alt=\"Scalar summary\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Histogram\n",
    "Histograms are useful for matrces and higher rank tensors. They show the distribution of values in each step and are often used to detect unexpected behavior of weights and biases of neural networks.\n",
    "\n",
    "Aditionally, the distributions tab shows the evolution of the percentiles of the values (max, 93%, 84%, 69%, 50%, 31%, 16%, 7%, min) over time/steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "b = tf.Variable(tf.random_uniform((50,30),-1,1), name='b')\n",
    "tf.summary.histogram(\"b_tensor_summary\", b)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    writer = tf.summary.FileWriter(\"tensorboard4\",session.graph)\n",
    "    for step in range(50):\n",
    "        session.run(tf.global_variables_initializer()) # do this on every step to get new random values\n",
    "        s_val = session.run(merged_summary)\n",
    "        writer.add_summary(s_val,step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "    <td> <img src=\"tensor_summary.png\" alt=\"Histogram summary\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"distributions.png\" alt=\"Distributions\" style=\"width: 400px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image\n",
    "[tf.summary.image](https://www.tensorflow.org/api_docs/python/tf/summary/image) accepts data as a 4-D tensor of shape (batch_size, height, width, channels) where channels can be:\n",
    "\n",
    "    1: Grayscale\n",
    "    3: RGB\n",
    "    4: RGBA\n",
    "\n",
    "Depending on the type of the input tensor data is handled as follows:\n",
    "* **uint8**: interpreted as is. Must be in the range 0-255\n",
    "* **float32**\n",
    "    * **positive**: scaled to 0-255\n",
    "    * **negative**: shifted to positive (centered at 127) and then scaled to 0-255\n",
    "    \n",
    "Batch size may be specified as -1 to use the entire batch. Then up to max_outputs images will be written.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other [summaries](https://www.tensorflow.org/api_guides/python/summary#Generation_of_Summaries), such as Audio, Text, [Embeddings projector (T-SNE,PCA)](https://www.tensorflow.org/guide/embedding) etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
